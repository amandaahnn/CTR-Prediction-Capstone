{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe7759e",
   "metadata": {},
   "source": [
    "## Preprocessing Training Data Development\n",
    "In this notebook we will create dummy features and/or one-hot encode features if necessary. Additionally, we will scale the data or apply standardization methods if necessary. Lastly, we are able to split our data into training and testing subsets to use for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7bf5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd1f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62591f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'site_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b4ae6",
   "metadata": {},
   "source": [
    "There is no need to create dummy features for my data because I am using the click column as my main variable, which is already numerical as 1 for when a click was present and 0 for when a click was not present. Creating dummy variables for any of the other columns wouldn't be helpful and would just make my data a lot larger when it's already very big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c5154a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "click                int64\n",
       "hour                 int64\n",
       "C1                   int64\n",
       "banner_pos           int64\n",
       "site_domain         object\n",
       "site_category       object\n",
       "app_id              object\n",
       "app_domain          object\n",
       "app_category        object\n",
       "device_id           object\n",
       "device_ip           object\n",
       "device_model        object\n",
       "device_type          int64\n",
       "device_conn_type     int64\n",
       "C14                  int64\n",
       "C15                  int64\n",
       "C16                  int64\n",
       "C17                  int64\n",
       "C18                  int64\n",
       "C19                  int64\n",
       "C20                  int64\n",
       "C21                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81592502",
   "metadata": {},
   "source": [
    "There is also no need to standardize my values because most of them are categorical and the rest are unknown variables (C14 - C21). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6f7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I will create my training and testing data splits. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "columns = ['hour', 'C1', 'banner_pos', 'device_type', 'device_conn_type', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'site_domain', 'site_category', 'app_id', 'app_domain', 'app_category', 'device_id', 'device_ip', 'device_model']\n",
    "X = data[columns]\n",
    "y = data.click\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5839ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32343173, 21), (8085794, 21), (32343173,), (8085794,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f9483",
   "metadata": {},
   "source": [
    "Looking at our testing and training subsets, they might be too large and unbalanced to efficiently and accurately run our models on. Therefore, we might need to sample the data to make it smaller and use an over sampling method to balance the amount of data in the 1 click vs. 0 click category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e79ef",
   "metadata": {},
   "source": [
    "### Sampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c79a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33563901, 22) (6865066, 22)\n"
     ]
    }
   ],
   "source": [
    "data_0 = data[data['click']==0]\n",
    "data_1 = data[data['click']==1]\n",
    "print(data_0.shape, data_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89f68fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335639, 22)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0 = data_0.sample(frac=0.01, random_state=42)\n",
    "data_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b090397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205952, 22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = data_1.sample(frac=0.03, random_state=42)\n",
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bffde39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    335639\n",
       "1    205952\n",
       "Name: click, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = pd.concat([data_0, data_1], axis = 0, ignore_index=True)\n",
    "data_sample['click'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181ab13",
   "metadata": {},
   "source": [
    "We won't apply the oversampling method yet because we want to use our original/organic data in our models first in order to see how they perform. If the performance metrices are lacking, we will use RandomOverSampler from the Imblearn library to balance the number of 0 and 1 clicks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
